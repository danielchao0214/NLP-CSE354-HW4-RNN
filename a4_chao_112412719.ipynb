{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"a4_chao_112412719.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMkKLOrTQePzZMrU8Y6tHxD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uT_R0wIZ9199"},"source":["# **GOOGLE DRIVE**"]},{"cell_type":"code","metadata":{"id":"VckLOWXdBof_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620865276198,"user_tz":240,"elapsed":307,"user":{"displayName":"Daniel Chao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggg3qYkal6TO7pIWs6_XannMrox-x7uGey3xC_t6Vw=s64","userId":"18118069315475813019"}},"outputId":"7fb9e19e-a2ed-45e6-aeba-07ddbc9f9aa9"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RIwl8vQ_Bzpj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620865276425,"user_tz":240,"elapsed":528,"user":{"displayName":"Daniel Chao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggg3qYkal6TO7pIWs6_XannMrox-x7uGey3xC_t6Vw=s64","userId":"18118069315475813019"}},"outputId":"7df9004a-978d-4fab-e49d-ff6cea3faf95"},"source":["%cd '/content/gdrive/MyDrive/Class/Year2/sem2/CSE354: Natural Language Processing/Homework4'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/Class/Year2/sem2/CSE354: Natural Language Processing/Homework4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cyd7tFXi52KM","executionInfo":{"status":"ok","timestamp":1620865278867,"user_tz":240,"elapsed":2961,"user":{"displayName":"Daniel Chao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggg3qYkal6TO7pIWs6_XannMrox-x7uGey3xC_t6Vw=s64","userId":"18118069315475813019"}},"outputId":"b7e935fe-9003-4fc3-b1d4-a86ab48efb0f"},"source":["pip install torch==1.4.0 torchvision==0.5.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.7/dist-packages (1.4.0)\n","Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.7/dist-packages (0.5.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QqpAxrAsAtA8"},"source":["# **Part 1**"]},{"cell_type":"markdown","metadata":{"id":"NJWeEFPx9gnc"},"source":["## **1.1**\n","\n"]},{"cell_type":"code","metadata":{"id":"jSYV0-7A5KmD"},"source":["import torch\n","import torch.nn as nn  #pytorch\n","import json #for reading json encoded files into opjects\n","import sys\n","import re #regular expressions\n","import numpy as np\n","import pandas as pd\n","import csv\n","from pprint import pprint\n","import gensim.downloader as api\n","from gensim.utils import tokenize\n","import torch.nn.functional as F\n","word_embs = api.load('glove-wiki-gigaword-50')\n","# sys.stdout = open('a4_lastname_id_OUTPUT.txt', 'w')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ib61nuUe6iXn"},"source":["def loadData(filename):\n","  data = []\n","  with open(filename, 'r') as infile:\n","    data = json.load(infile)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8h7tvNvv9ngs"},"source":["## **1.2**"]},{"cell_type":"code","metadata":{"id":"ETQfGUz48bu3"},"source":["def tokenize_data(data):\n","  for record in data:\n","    record['question_toks'] = list(tokenize(record['question'], lowercase=True))\n","    record['passage_toks'] = list(tokenize(record['passage'], lowercase=True))\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNTUcVwaBpQ8"},"source":["def get_embed(word):\n","  if word in word_embs.vocab:\n","    return word_embs.word_vec(word)\n","  else:\n","    return word_embs.word_vec('unk')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYLpuAH5CPal"},"source":["def get_inputs(data):\n","  input = []\n","  for record in data:\n","    x = list(np.array(list(get_embed(word))) for word in list(record['passage_toks']) + list(record['question_toks']))\n","    x = torch.from_numpy(np.array(x))\n","    input.append(x)\n","  return input"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tKyK665i9qUA"},"source":["## **1.3**"]},{"cell_type":"code","metadata":{"id":"U7LfHrKGD7Xk"},"source":["class GRU_RNN(nn.Module):\n","\n","  def __init__(self, embedding_dim=50, gru_hidden_dim=50, number_of_labels=2):\n","    super(GRU_RNN, self).__init__()\n","    self.gru = nn.GRU(embedding_dim, gru_hidden_dim)\n","    self.linearClassifier = nn.Linear(gru_hidden_dim, number_of_labels)\n","        \n","  def forward(self, X):        \n","    vecs = []\n","    for doc in X:\n","      s, _ = self.gru(doc.unsqueeze(1)) \n","      vecs.append(s[-1])\n","    vecs = torch.stack(vecs).squeeze(1)  \n","    vecs = self.linearClassifier(vecs)\n","    yprobs = F.softmax(vecs, dim=1)\n","    return yprobs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1Ltd6-t7RyC"},"source":["train_data = loadData('music_QA_train.json')\n","trial_data = loadData('music_QA_dev.json')\n","test_data = loadData('music_QA_test.json')\n","\n","train_data = tokenize_data(train_data)\n","trial_data = tokenize_data(trial_data)\n","test_data = tokenize_data(test_data)\n","\n","train_input = get_inputs(train_data)\n","train_output = torch.from_numpy(np.array([[0,1] if x['label'] else [1,0] for x in train_data]))\n","train_output = train_output.type(torch.FloatTensor)\n","\n","\n","#Model setup:\n","learning_rate, epochs = 0.1, 10\n","model = GRU_RNN(50, 50, 2)\n","sgd = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","lossfunc = nn.BCELoss()\n","\n","# # training loop:\n","for i in range(epochs):\n","  model.train()\n","  sgd.zero_grad()\n","  #forward pass:\n","  ypred = model(train_input)\n","  loss = lossfunc(ypred, train_output)\n","  #backward:\n","  loss.backward()\n","  sgd.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0CJ2fx_N9t_k"},"source":["## **1.4**"]},{"cell_type":"code","metadata":{"id":"JUsayH3OZB7u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620865643690,"user_tz":240,"elapsed":103837,"user":{"displayName":"Daniel Chao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggg3qYkal6TO7pIWs6_XannMrox-x7uGey3xC_t6Vw=s64","userId":"18118069315475813019"}},"outputId":"f281790c-9c87-42f4-d2d7-2bb35638d2a6"},"source":["trial_input = get_inputs(trial_data)\n","trial_output = torch.from_numpy(np.array([[0,1] if x['label'] else [1,0] for x in trial_data]))\n","trial_y = torch.from_numpy(np.array([1 if x['label'] else 0 for x in trial_data]))\n","trial_output = trial_output.type(torch.FloatTensor)\n","\n","with torch.no_grad(): \n","  trialpred_prob = model(trial_input).numpy()\n","  trialpred_class = [np.where(prob == max(prob))[0][0] for prob in trialpred_prob]\n","  count = np.sum(np.array(trialpred_class) == np.array(trial_y))\n","  print(\"TRIAL DATA CORRECT: \", count, \" out of \", len(trial_y))\n","  print(\"TRIAL DATA ACCURACY: \", count/len(trial_y)*100, \"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TRIAL DATA CORRECT:  61  out of  85\n","TRIAL DATA ACCURACY:  71.76470588235294 %\n"],"name":"stdout"}]}]}